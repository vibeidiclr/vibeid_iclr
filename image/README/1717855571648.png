{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom collections import Counter\nimport os\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets, models\nimport torchvision.transforms.functional as TF\nimport torchvision.models as models\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:32:34.663873Z","iopub.execute_input":"2024-06-07T13:32:34.664219Z","iopub.status.idle":"2024-06-07T13:32:39.978152Z","shell.execute_reply.started":"2024-06-07T13:32:34.664185Z","shell.execute_reply":"2024-06-07T13:32:39.977154Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Create mutli-channel input to the model","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n# Define transformations for the training and testing sets\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\ndef load_images_from_folder(data_dir, num_channels, width, height, labels):\n  \"\"\"Loads images from a folder with 5 multichannel inputs.\n\n  Args:\n    data_dir: Path to the data directory.\n    num_channels: Number of channels for each image (1 for grayscale, 3 for RGB).\n    width: Target width for resizing images.\n    height: Target height for resizing images.\n    labels: List of class labels.\n\n  Returns:\n    X: A numpy array of shape (num_images, num_channels * 5, height, width).\n    y: A numpy array of shape (num_images,) containing class labels.\n  \"\"\"\n\n  X = []\n  y = []\n  extension = [\".png\", \".jpg\", \".jpeg\"]\n  for label_index, label_dir in enumerate(sorted(os.listdir(data_dir))):\n    if not label_dir.startswith(\".\"):\n      label_path = os.path.join(data_dir, label_dir)\n      i = 0\n      channel_data = []  # List to store data for all 5 channels\n\n      for fname in sorted(os.listdir(label_path)):\n        if any(fname.endswith(ext) for ext in extension) and not fname.startswith(\".\"):\n          i += 1\n          path = os.path.join(label_path, fname)\n          img = cv2.imread(path, cv2.IMREAD_COLOR if num_channels == 3 else cv2.IMREAD_GRAYSCALE)\n          img = cv2.resize(img, (width, height))\n          channel_data.append(img)\n\n          # Check if all 5 channels are loaded, then process and append\n          if i % 5 == 0:\n            if num_channels == 1:\n              feature = np.stack(channel_data, axis=-1)\n            elif num_channels == 3:\n              feature = np.concatenate(channel_data, axis=-1)\n            # Reshape to (num_channels * 5, height, width) before appending to X\n            feature = np.transpose(feature, (2, 0, 1))\n            X.append(feature)\n            y.append(label_index)\n            channel_data = []  # Reset channel data list\n\n  return np.asarray(X), np.asarray(y)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:33:21.090844Z","iopub.execute_input":"2024-06-07T13:33:21.091676Z","iopub.status.idle":"2024-06-07T13:33:21.103055Z","shell.execute_reply.started":"2024-06-07T13:33:21.091643Z","shell.execute_reply":"2024-06-07T13:33:21.102010Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Source Data","metadata":{}},{"cell_type":"code","source":"A2_1train_dir =\"/kaggle/input/vibeid-a2/VIBeID_A2/VIBeID_A2_1/train\"\nA2_1test_dir = \"/kaggle/input/vibeid-a2/VIBeID_A2/VIBeID_A2_1/test\"\nA2_1val_dir = \"/kaggle/input/vibeid-a2/VIBeID_A2/VIBeID_A2_1/val\"","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:33:41.287988Z","iopub.execute_input":"2024-06-07T13:33:41.288347Z","iopub.status.idle":"2024-06-07T13:33:41.293524Z","shell.execute_reply.started":"2024-06-07T13:33:41.288317Z","shell.execute_reply":"2024-06-07T13:33:41.292558Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"num_channels = 3\nwidth = 128\nheight =128\nnum_classes = 40\nlabels = sorted(os.listdir(A2_1train_dir))[:num_classes]  # Assuming labels are sorted alphabetically\nprint(labels)\nX_train, y_train = load_images_from_folder(A2_1train_dir, num_channels, width, height, labels)\nX_test, y_test = load_images_from_folder(A2_1test_dir, num_channels, width, height, labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:33:42.959245Z","iopub.execute_input":"2024-06-07T13:33:42.960117Z","iopub.status.idle":"2024-06-07T13:36:43.723059Z","shell.execute_reply.started":"2024-06-07T13:33:42.960084Z","shell.execute_reply":"2024-06-07T13:36:43.722281Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '4', '5', '6', '7', '8', '9']\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Assuming X_train, y_train, X_test, and y_test are numpy arrays or PyTorch tensors\n\n# Convert train and test data into PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Assuming y_train is categorical\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)  # Assuming y_test is categorical\nnum_epochs = 10\nbatch_size = 16\n# Create DataLoaders with rearranged data\ntrain_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:38:18.490441Z","iopub.execute_input":"2024-06-07T13:38:18.491289Z","iopub.status.idle":"2024-06-07T13:38:20.604519Z","shell.execute_reply.started":"2024-06-07T13:38:18.491255Z","shell.execute_reply":"2024-06-07T13:38:20.603291Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(\"Train data shape (X_train):\", X_train_tensor.shape)\nprint(\"Train data labels shape (y_train):\", y_train_tensor.shape)\n\nprint(\"Test data shape (X_test):\", X_test_tensor.shape)\nprint(\"Test data labels shape (y_test):\", y_test_tensor.shape)\nimport torch\nfrom collections import Counter\n\n# Assuming y_train_tensor and y_test_tensor are already defined as tensors of labels\n\n# Count the number of samples per class in the training set\ntrain_class_counts = Counter(y_train_tensor.numpy())\nprint(\"Training set class distribution:\")\nfor class_label, count in train_class_counts.items():\n    print(f\"Class {class_label}: {count} samples\")\n\n# Count the number of samples per class in the test set\ntest_class_counts = Counter(y_test_tensor.numpy())\nprint(\"\\nTest set class distribution:\")\nfor class_label, count in test_class_counts.items():\n    print(f\"Class {class_label}: {count} samples\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:38:21.598798Z","iopub.execute_input":"2024-06-07T13:38:21.599531Z","iopub.status.idle":"2024-06-07T13:38:21.614121Z","shell.execute_reply.started":"2024-06-07T13:38:21.599491Z","shell.execute_reply":"2024-06-07T13:38:21.613166Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Train data shape (X_train): torch.Size([4626, 15, 128, 128])\nTrain data labels shape (y_train): torch.Size([4626])\nTest data shape (X_test): torch.Size([1316, 15, 128, 128])\nTest data labels shape (y_test): torch.Size([1316])\nTraining set class distribution:\nClass 0: 161 samples\nClass 1: 153 samples\nClass 2: 98 samples\nClass 3: 146 samples\nClass 4: 159 samples\nClass 5: 163 samples\nClass 6: 179 samples\nClass 7: 154 samples\nClass 8: 157 samples\nClass 9: 204 samples\nClass 10: 107 samples\nClass 11: 144 samples\nClass 12: 166 samples\nClass 13: 181 samples\nClass 14: 163 samples\nClass 15: 156 samples\nClass 16: 151 samples\nClass 17: 121 samples\nClass 18: 102 samples\nClass 19: 168 samples\nClass 20: 168 samples\nClass 21: 131 samples\nClass 22: 169 samples\nClass 23: 173 samples\nClass 24: 146 samples\nClass 25: 108 samples\nClass 26: 179 samples\nClass 27: 213 samples\nClass 28: 132 samples\nClass 29: 174 samples\n\nTest set class distribution:\nClass 0: 46 samples\nClass 1: 43 samples\nClass 2: 28 samples\nClass 3: 42 samples\nClass 4: 45 samples\nClass 5: 46 samples\nClass 6: 51 samples\nClass 7: 43 samples\nClass 8: 45 samples\nClass 9: 58 samples\nClass 10: 30 samples\nClass 11: 41 samples\nClass 12: 47 samples\nClass 13: 52 samples\nClass 14: 46 samples\nClass 15: 44 samples\nClass 16: 43 samples\nClass 17: 34 samples\nClass 18: 29 samples\nClass 19: 48 samples\nClass 20: 48 samples\nClass 21: 37 samples\nClass 22: 48 samples\nClass 23: 49 samples\nClass 24: 42 samples\nClass 25: 31 samples\nClass 26: 51 samples\nClass 27: 61 samples\nClass 28: 38 samples\nClass 29: 50 samples\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train Source Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets, models\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass CustomResNet(nn.Module):\n    def __init__(self):\n        super(CustomResNet, self).__init__()\n        resnet =  models.resnet50(pretrained=False)\n        for param in resnet.parameters():\n          param.requires_grad = True\n        resnet.conv1 =  nn.Conv2d(15, 64, kernel_size=7, stride=2, padding=3)\n        num_ftrs = resnet.fc.in_features\n        resnet.fc = nn.Sequential(nn.Linear(num_ftrs,30))\n        self.resnet = resnet\n    def forward(self, x):\n        return self.resnet(x)\ncustom_resnet = CustomResNet()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(custom_resnet.parameters())\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=3, verbose=True)\noptimizer =  optim.Adam(custom_resnet.parameters())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncustom_resnet.to(device)\nfrom torch.optim import lr_scheduler\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\nnum_epochs = 100\ndef train_and_test_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs):\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()  # Set the model to training mode\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_acc = correct / total\n\n#         print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}\")\n\n        # Testing phase\n        model.eval()  # Set the model to evaluation mode\n        test_correct = 0\n        test_total = 0\n        test_loss = 0.0\n\n        with torch.no_grad():  # Enable gradient computation during inference\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                test_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                test_total += labels.size(0)\n                test_correct += (predicted == labels).sum().item()\n\n        test_loss /= len(test_loader)\n        test_accuracy = test_correct / test_total\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n\n        # Update learning rate scheduler\n        if scheduler is not None:\n            scheduler.step(test_loss)\n\n    print(\"Training completed!\")\n# Train and test the model\ntrain_and_test_model(custom_resnet, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs)\nprint(\"Training completed!\")","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:48:33.013583Z","iopub.execute_input":"2024-06-07T13:48:33.014468Z","iopub.status.idle":"2024-06-07T14:16:43.693185Z","shell.execute_reply.started":"2024-06-07T13:48:33.014433Z","shell.execute_reply":"2024-06-07T14:16:43.692183Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch [1/100], Test Loss: 2.1932, Test Accuracy: 0.3093\nEpoch [2/100], Test Loss: 1.3721, Test Accuracy: 0.5418\nEpoch [3/100], Test Loss: 1.0932, Test Accuracy: 0.6185\nEpoch [4/100], Test Loss: 0.8214, Test Accuracy: 0.7029\nEpoch [5/100], Test Loss: 0.8788, Test Accuracy: 0.6892\nEpoch [6/100], Test Loss: 0.7857, Test Accuracy: 0.7173\nEpoch [7/100], Test Loss: 0.7520, Test Accuracy: 0.7204\nEpoch [8/100], Test Loss: 0.7713, Test Accuracy: 0.7150\nEpoch [9/100], Test Loss: 0.6597, Test Accuracy: 0.7660\nEpoch [10/100], Test Loss: 0.6841, Test Accuracy: 0.7492\nEpoch [11/100], Test Loss: 0.6613, Test Accuracy: 0.7667\nEpoch [12/100], Test Loss: 0.8502, Test Accuracy: 0.7295\nEpoch [13/100], Test Loss: 0.5794, Test Accuracy: 0.8024\nEpoch [14/100], Test Loss: 0.6143, Test Accuracy: 0.8123\nEpoch [15/100], Test Loss: 0.9073, Test Accuracy: 0.7401\nEpoch [16/100], Test Loss: 0.6538, Test Accuracy: 0.7994\nEpoch [17/100], Test Loss: 0.5305, Test Accuracy: 0.8275\nEpoch [18/100], Test Loss: 0.5434, Test Accuracy: 0.8222\nEpoch [19/100], Test Loss: 0.6544, Test Accuracy: 0.8024\nEpoch [20/100], Test Loss: 0.6310, Test Accuracy: 0.8207\nEpoch [21/100], Test Loss: 0.7083, Test Accuracy: 0.7918\nEpoch 00021: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [22/100], Test Loss: 0.4557, Test Accuracy: 0.8556\nEpoch [23/100], Test Loss: 0.4605, Test Accuracy: 0.8655\nEpoch [24/100], Test Loss: 0.4685, Test Accuracy: 0.8602\nEpoch [25/100], Test Loss: 0.4814, Test Accuracy: 0.8625\nEpoch [26/100], Test Loss: 0.4933, Test Accuracy: 0.8617\nEpoch 00026: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [27/100], Test Loss: 0.5065, Test Accuracy: 0.8617\nEpoch [28/100], Test Loss: 0.4823, Test Accuracy: 0.8640\nEpoch [29/100], Test Loss: 0.4944, Test Accuracy: 0.8640\nEpoch [30/100], Test Loss: 0.4960, Test Accuracy: 0.8602\nEpoch 00030: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [31/100], Test Loss: 0.5229, Test Accuracy: 0.8602\nEpoch [32/100], Test Loss: 0.4949, Test Accuracy: 0.8587\nEpoch [33/100], Test Loss: 0.5105, Test Accuracy: 0.8579\nEpoch [34/100], Test Loss: 0.5010, Test Accuracy: 0.8617\nEpoch 00034: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [35/100], Test Loss: 0.5091, Test Accuracy: 0.8587\nEpoch [36/100], Test Loss: 0.5179, Test Accuracy: 0.8594\nEpoch [37/100], Test Loss: 0.4876, Test Accuracy: 0.8632\nEpoch [38/100], Test Loss: 0.5058, Test Accuracy: 0.8571\nEpoch 00038: reducing learning rate of group 0 to 1.0000e-08.\nEpoch [39/100], Test Loss: 0.4952, Test Accuracy: 0.8594\nEpoch [40/100], Test Loss: 0.5226, Test Accuracy: 0.8564\nEpoch [41/100], Test Loss: 0.4905, Test Accuracy: 0.8625\nEpoch [42/100], Test Loss: 0.5006, Test Accuracy: 0.8609\nEpoch [43/100], Test Loss: 0.4897, Test Accuracy: 0.8647\nEpoch [44/100], Test Loss: 0.5043, Test Accuracy: 0.8625\nEpoch [45/100], Test Loss: 0.4999, Test Accuracy: 0.8602\nEpoch [46/100], Test Loss: 0.5014, Test Accuracy: 0.8609\nEpoch [47/100], Test Loss: 0.4765, Test Accuracy: 0.8655\nEpoch [48/100], Test Loss: 0.5069, Test Accuracy: 0.8571\nEpoch [49/100], Test Loss: 0.4747, Test Accuracy: 0.8647\nEpoch [50/100], Test Loss: 0.4895, Test Accuracy: 0.8640\nEpoch [51/100], Test Loss: 0.4883, Test Accuracy: 0.8602\nEpoch [52/100], Test Loss: 0.4923, Test Accuracy: 0.8609\nEpoch [53/100], Test Loss: 0.4854, Test Accuracy: 0.8640\nEpoch [54/100], Test Loss: 0.4916, Test Accuracy: 0.8640\nEpoch [55/100], Test Loss: 0.5092, Test Accuracy: 0.8556\nEpoch [56/100], Test Loss: 0.4969, Test Accuracy: 0.8602\nEpoch [57/100], Test Loss: 0.5052, Test Accuracy: 0.8647\nEpoch [58/100], Test Loss: 0.5125, Test Accuracy: 0.8579\nEpoch [59/100], Test Loss: 0.5148, Test Accuracy: 0.8594\nEpoch [60/100], Test Loss: 0.5005, Test Accuracy: 0.8625\nEpoch [61/100], Test Loss: 0.5078, Test Accuracy: 0.8602\nEpoch [62/100], Test Loss: 0.5051, Test Accuracy: 0.8594\nEpoch [63/100], Test Loss: 0.5061, Test Accuracy: 0.8602\nEpoch [64/100], Test Loss: 0.5013, Test Accuracy: 0.8647\nEpoch [65/100], Test Loss: 0.5039, Test Accuracy: 0.8564\nEpoch [66/100], Test Loss: 0.4917, Test Accuracy: 0.8640\nEpoch [67/100], Test Loss: 0.4977, Test Accuracy: 0.8632\nEpoch [68/100], Test Loss: 0.5078, Test Accuracy: 0.8579\nEpoch [69/100], Test Loss: 0.4912, Test Accuracy: 0.8655\nEpoch [70/100], Test Loss: 0.5009, Test Accuracy: 0.8587\nEpoch [71/100], Test Loss: 0.5016, Test Accuracy: 0.8655\nEpoch [72/100], Test Loss: 0.5109, Test Accuracy: 0.8594\nEpoch [73/100], Test Loss: 0.5172, Test Accuracy: 0.8594\nEpoch [74/100], Test Loss: 0.4877, Test Accuracy: 0.8571\nEpoch [75/100], Test Loss: 0.5101, Test Accuracy: 0.8602\nEpoch [76/100], Test Loss: 0.4892, Test Accuracy: 0.8625\nEpoch [77/100], Test Loss: 0.5032, Test Accuracy: 0.8579\nEpoch [78/100], Test Loss: 0.5083, Test Accuracy: 0.8549\nEpoch [79/100], Test Loss: 0.5047, Test Accuracy: 0.8602\nEpoch [80/100], Test Loss: 0.5128, Test Accuracy: 0.8625\nEpoch [81/100], Test Loss: 0.5227, Test Accuracy: 0.8587\nEpoch [82/100], Test Loss: 0.5130, Test Accuracy: 0.8549\nEpoch [83/100], Test Loss: 0.4976, Test Accuracy: 0.8594\nEpoch [84/100], Test Loss: 0.5042, Test Accuracy: 0.8663\nEpoch [85/100], Test Loss: 0.5012, Test Accuracy: 0.8663\nEpoch [86/100], Test Loss: 0.4813, Test Accuracy: 0.8632\nEpoch [87/100], Test Loss: 0.5067, Test Accuracy: 0.8602\nEpoch [88/100], Test Loss: 0.5025, Test Accuracy: 0.8632\nEpoch [89/100], Test Loss: 0.4894, Test Accuracy: 0.8617\nEpoch [90/100], Test Loss: 0.5016, Test Accuracy: 0.8602\nEpoch [91/100], Test Loss: 0.5019, Test Accuracy: 0.8647\nEpoch [92/100], Test Loss: 0.5056, Test Accuracy: 0.8602\nEpoch [93/100], Test Loss: 0.5071, Test Accuracy: 0.8571\nEpoch [94/100], Test Loss: 0.4842, Test Accuracy: 0.8640\nEpoch [95/100], Test Loss: 0.4957, Test Accuracy: 0.8655\nEpoch [96/100], Test Loss: 0.5101, Test Accuracy: 0.8594\nEpoch [97/100], Test Loss: 0.5064, Test Accuracy: 0.8609\nEpoch [98/100], Test Loss: 0.4937, Test Accuracy: 0.8594\nEpoch [99/100], Test Loss: 0.4870, Test Accuracy: 0.8647\nEpoch [100/100], Test Loss: 0.4913, Test Accuracy: 0.8609\nTraining completed!\nTraining completed!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"torch.save(custom_resnet.state_dict(), 'VIBeID_A2_1.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:16:43.695111Z","iopub.execute_input":"2024-06-07T14:16:43.695477Z","iopub.status.idle":"2024-06-07T14:16:43.932698Z","shell.execute_reply.started":"2024-06-07T14:16:43.695444Z","shell.execute_reply":"2024-06-07T14:16:43.931660Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Load Target Data","metadata":{}},{"cell_type":"code","source":"A2_2test_dir = \"/kaggle/input/vibeid-a2/VIBeID_A2/VIBeID_A2_2/test\"\nA2_2val_dir = \"/kaggle/input/vibeid-a2/VIBeID_A2/VIBeID_A2_2/val\"\n\nA2_3test_dir = \"/kaggle/input/vibeid-a2/VIBeID_A2/VIBeID_A2_3/test\"\nA2_3val_dir = \"/kaggle/input/vibeid-a2/VIBeID_A2/VIBeID_A2_3/val\"","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:16:43.933995Z","iopub.execute_input":"2024-06-07T14:16:43.934306Z","iopub.status.idle":"2024-06-07T14:16:43.939376Z","shell.execute_reply.started":"2024-06-07T14:16:43.934280Z","shell.execute_reply":"2024-06-07T14:16:43.938436Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntarget_train_dir = A2_2test_dir\ntarget_test_dir = A2_2val_dir \n\nnum_channels = 3\nwidth = 128\nheight = 128\nnum_classes = 30\n\ntarget_X_train, target_y_train = load_images_from_folder(target_train_dir, num_channels, width, height, labels)\ntarget_X_test, target_y_test = load_images_from_folder(target_test_dir, num_channels, width, height, labels)\n\n# Convert train and test data into PyTorch tensors\ntarget_X_train_tensor = torch.tensor(target_X_train, dtype=torch.float32)\ntarget_y_train_tensor = torch.tensor(target_y_train, dtype=torch.long)  # Assuming y_train is categorical\ntarget_X_test_tensor = torch.tensor(target_X_test, dtype=torch.float32)\ntarget_y_test_tensor = torch.tensor(target_y_test, dtype=torch.long)  # Assuming y_test is categorical\n\nnum_epochs = 10\nbatch_size = 16\n\n# Create DataLoaders with rearranged data\ntarget_train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(target_X_train_tensor,  target_y_train_tensor), batch_size=batch_size, shuffle=True)\ntarget_test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(target_X_test_tensor,target_y_test_tensor), batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:16:43.941929Z","iopub.execute_input":"2024-06-07T14:16:43.942292Z","iopub.status.idle":"2024-06-07T14:17:05.070353Z","shell.execute_reply.started":"2024-06-07T14:16:43.942260Z","shell.execute_reply":"2024-06-07T14:17:05.069284Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Test Target--> Source Accuracy","metadata":{}},{"cell_type":"code","source":"running_loss = 0.0\ncorrect = 0\ntotal = 0\ntest_correct = 0\ntest_total = 0\ntest_loss = 0.0\nwith torch.no_grad():  # No need to compute gradients during inference\n    for images, labels in target_test_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs =custom_resnet(images)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)\n        test_total += labels.size(0)\n        test_correct += (predicted == labels).sum().item()\n\ntest_loss /= len(test_loader)\ntest_accuracy = test_correct / test_total\n\nprint(f\" Test Loss: {test_loss:.4f},source Accuracy: {test_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:17:05.071825Z","iopub.execute_input":"2024-06-07T14:17:05.072622Z","iopub.status.idle":"2024-06-07T14:17:05.504744Z","shell.execute_reply.started":"2024-06-07T14:17:05.072585Z","shell.execute_reply":"2024-06-07T14:17:05.503797Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":" Test Loss: 2.8307,source Accuracy: 0.1780\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tune Unfreeze the last 3 layers","metadata":{}},{"cell_type":"code","source":"loaded_model = CustomResNet()\n# print(loaded_model)\nloaded_model.to(device)\n\n# Move the criterion to the same device as the data\ncriterion.to(device)\n# Load the saved weights\nloaded_model.load_state_dict(torch.load('/kaggle/working/VIBeID_A2_1.pth'))\n\n# Unfreeze the last 3 layers\nfor param in loaded_model.resnet.fc.parameters():\n    param.requires_grad = True\nfor name, param in loaded_model.named_parameters():\n    if 'resnet.layer4' in name or 'resnet.fc' in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(loaded_model.parameters())\n\n# Define a learning rate scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=3, verbose=True)\n\ntrain_and_test_model(loaded_model,target_train_loader , target_test_loader, criterion, optimizer, scheduler, device, num_epochs=50)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:17:37.853873Z","iopub.execute_input":"2024-06-07T14:17:37.854554Z","iopub.status.idle":"2024-06-07T14:19:10.619296Z","shell.execute_reply.started":"2024-06-07T14:17:37.854514Z","shell.execute_reply":"2024-06-07T14:19:10.618349Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch [1/50], Test Loss: 1.4067, Test Accuracy: 0.5314\nEpoch [2/50], Test Loss: 0.9931, Test Accuracy: 0.6911\nEpoch [3/50], Test Loss: 0.8514, Test Accuracy: 0.7225\nEpoch [4/50], Test Loss: 0.7658, Test Accuracy: 0.7592\nEpoch [5/50], Test Loss: 0.7585, Test Accuracy: 0.7670\nEpoch [6/50], Test Loss: 0.8089, Test Accuracy: 0.7618\nEpoch [7/50], Test Loss: 0.7493, Test Accuracy: 0.7696\nEpoch [8/50], Test Loss: 0.7089, Test Accuracy: 0.7853\nEpoch [9/50], Test Loss: 0.8231, Test Accuracy: 0.7670\nEpoch [10/50], Test Loss: 0.8716, Test Accuracy: 0.7801\nEpoch [11/50], Test Loss: 0.7939, Test Accuracy: 0.7749\nEpoch [12/50], Test Loss: 0.9475, Test Accuracy: 0.7749\nEpoch 00012: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [13/50], Test Loss: 0.9396, Test Accuracy: 0.7827\nEpoch [14/50], Test Loss: 0.8971, Test Accuracy: 0.7801\nEpoch [15/50], Test Loss: 0.8933, Test Accuracy: 0.7906\nEpoch [16/50], Test Loss: 0.8911, Test Accuracy: 0.7853\nEpoch 00016: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [17/50], Test Loss: 0.8796, Test Accuracy: 0.7932\nEpoch [18/50], Test Loss: 0.8774, Test Accuracy: 0.7906\nEpoch [19/50], Test Loss: 0.8844, Test Accuracy: 0.7958\nEpoch [20/50], Test Loss: 0.8652, Test Accuracy: 0.7880\nEpoch 00020: reducing learning rate of group 0 to 1.0000e-09.\nEpoch [21/50], Test Loss: 0.8834, Test Accuracy: 0.7932\nEpoch [22/50], Test Loss: 0.9097, Test Accuracy: 0.7984\nEpoch [23/50], Test Loss: 0.8666, Test Accuracy: 0.7958\nEpoch [24/50], Test Loss: 0.8909, Test Accuracy: 0.7880\nEpoch [25/50], Test Loss: 0.8529, Test Accuracy: 0.7880\nEpoch [26/50], Test Loss: 0.8821, Test Accuracy: 0.8010\nEpoch [27/50], Test Loss: 0.8707, Test Accuracy: 0.7932\nEpoch [28/50], Test Loss: 0.8861, Test Accuracy: 0.7906\nEpoch [29/50], Test Loss: 0.8640, Test Accuracy: 0.7853\nEpoch [30/50], Test Loss: 0.8875, Test Accuracy: 0.7880\nEpoch [31/50], Test Loss: 0.8701, Test Accuracy: 0.7958\nEpoch [32/50], Test Loss: 0.8481, Test Accuracy: 0.7932\nEpoch [33/50], Test Loss: 0.8759, Test Accuracy: 0.8037\nEpoch [34/50], Test Loss: 0.8860, Test Accuracy: 0.7932\nEpoch [35/50], Test Loss: 0.8848, Test Accuracy: 0.7932\nEpoch [36/50], Test Loss: 0.8895, Test Accuracy: 0.7958\nEpoch [37/50], Test Loss: 0.8690, Test Accuracy: 0.7958\nEpoch [38/50], Test Loss: 0.8646, Test Accuracy: 0.7906\nEpoch [39/50], Test Loss: 0.8742, Test Accuracy: 0.7906\nEpoch [40/50], Test Loss: 0.8797, Test Accuracy: 0.7880\nEpoch [41/50], Test Loss: 0.8787, Test Accuracy: 0.8010\nEpoch [42/50], Test Loss: 0.8660, Test Accuracy: 0.7906\nEpoch [43/50], Test Loss: 0.8740, Test Accuracy: 0.7984\nEpoch [44/50], Test Loss: 0.8718, Test Accuracy: 0.7958\nEpoch [45/50], Test Loss: 0.8610, Test Accuracy: 0.7906\nEpoch [46/50], Test Loss: 0.8548, Test Accuracy: 0.7906\nEpoch [47/50], Test Loss: 0.9207, Test Accuracy: 0.7984\nEpoch [48/50], Test Loss: 0.8652, Test Accuracy: 0.8037\nEpoch [49/50], Test Loss: 0.8983, Test Accuracy: 0.7880\nEpoch [50/50], Test Loss: 0.8654, Test Accuracy: 0.7932\nTraining completed!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tune Unfreeze all layers","metadata":{}},{"cell_type":"code","source":"loaded_model = CustomResNet()\n# print(loaded_model)\nloaded_model.to(device)\n\n# Move the criterion to the same device as the data\ncriterion.to(device)\n# Load the saved weights\nloaded_model.load_state_dict(torch.load('/kaggle/working/VIBeID_A2_1.pth'))\n\nfor param in loaded_model.resnet.parameters():\n    param.requires_grad = True\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(loaded_model.parameters())\n\n# Define a learning rate scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=3, verbose=True)\n\ntrain_and_test_model(loaded_model,target_train_loader , target_test_loader, criterion, optimizer, scheduler, device, num_epochs=50)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:19:10.621103Z","iopub.execute_input":"2024-06-07T14:19:10.621580Z","iopub.status.idle":"2024-06-07T14:21:40.014920Z","shell.execute_reply.started":"2024-06-07T14:19:10.621543Z","shell.execute_reply":"2024-06-07T14:21:40.013987Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch [1/50], Test Loss: 1.2223, Test Accuracy: 0.6047\nEpoch [2/50], Test Loss: 0.9986, Test Accuracy: 0.6597\nEpoch [3/50], Test Loss: 1.0260, Test Accuracy: 0.6545\nEpoch [4/50], Test Loss: 1.0803, Test Accuracy: 0.6780\nEpoch [5/50], Test Loss: 0.9527, Test Accuracy: 0.7251\nEpoch [6/50], Test Loss: 1.1602, Test Accuracy: 0.6832\nEpoch [7/50], Test Loss: 0.9885, Test Accuracy: 0.7251\nEpoch [8/50], Test Loss: 1.3687, Test Accuracy: 0.6649\nEpoch [9/50], Test Loss: 0.9855, Test Accuracy: 0.7408\nEpoch 00009: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [10/50], Test Loss: 0.9973, Test Accuracy: 0.7408\nEpoch [11/50], Test Loss: 0.9654, Test Accuracy: 0.7382\nEpoch [12/50], Test Loss: 0.9435, Test Accuracy: 0.7408\nEpoch [13/50], Test Loss: 0.9276, Test Accuracy: 0.7487\nEpoch [14/50], Test Loss: 0.9078, Test Accuracy: 0.7461\nEpoch [15/50], Test Loss: 0.8983, Test Accuracy: 0.7539\nEpoch [16/50], Test Loss: 0.8966, Test Accuracy: 0.7487\nEpoch [17/50], Test Loss: 0.8984, Test Accuracy: 0.7513\nEpoch [18/50], Test Loss: 0.8508, Test Accuracy: 0.7618\nEpoch [19/50], Test Loss: 0.8519, Test Accuracy: 0.7749\nEpoch [20/50], Test Loss: 0.8558, Test Accuracy: 0.7723\nEpoch [21/50], Test Loss: 0.8520, Test Accuracy: 0.7513\nEpoch [22/50], Test Loss: 0.8658, Test Accuracy: 0.7618\nEpoch 00022: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [23/50], Test Loss: 0.8714, Test Accuracy: 0.7565\nEpoch [24/50], Test Loss: 0.8679, Test Accuracy: 0.7539\nEpoch [25/50], Test Loss: 0.8512, Test Accuracy: 0.7723\nEpoch [26/50], Test Loss: 0.8603, Test Accuracy: 0.7696\nEpoch 00026: reducing learning rate of group 0 to 1.0000e-09.\nEpoch [27/50], Test Loss: 0.8743, Test Accuracy: 0.7670\nEpoch [28/50], Test Loss: 0.8873, Test Accuracy: 0.7644\nEpoch [29/50], Test Loss: 0.8847, Test Accuracy: 0.7644\nEpoch [30/50], Test Loss: 0.8567, Test Accuracy: 0.7670\nEpoch [31/50], Test Loss: 0.8520, Test Accuracy: 0.7644\nEpoch [32/50], Test Loss: 0.8722, Test Accuracy: 0.7565\nEpoch [33/50], Test Loss: 0.8887, Test Accuracy: 0.7461\nEpoch [34/50], Test Loss: 0.8515, Test Accuracy: 0.7670\nEpoch [35/50], Test Loss: 0.8599, Test Accuracy: 0.7565\nEpoch [36/50], Test Loss: 0.8544, Test Accuracy: 0.7723\nEpoch [37/50], Test Loss: 0.8882, Test Accuracy: 0.7723\nEpoch [38/50], Test Loss: 0.8502, Test Accuracy: 0.7644\nEpoch [39/50], Test Loss: 0.8622, Test Accuracy: 0.7696\nEpoch [40/50], Test Loss: 0.8706, Test Accuracy: 0.7618\nEpoch [41/50], Test Loss: 0.8589, Test Accuracy: 0.7592\nEpoch [42/50], Test Loss: 0.8498, Test Accuracy: 0.7670\nEpoch [43/50], Test Loss: 0.8619, Test Accuracy: 0.7670\nEpoch [44/50], Test Loss: 0.8939, Test Accuracy: 0.7487\nEpoch [45/50], Test Loss: 0.8522, Test Accuracy: 0.7670\nEpoch [46/50], Test Loss: 0.8598, Test Accuracy: 0.7670\nEpoch [47/50], Test Loss: 0.8588, Test Accuracy: 0.7644\nEpoch [48/50], Test Loss: 0.8517, Test Accuracy: 0.7696\nEpoch [49/50], Test Loss: 0.8571, Test Accuracy: 0.7670\nEpoch [50/50], Test Loss: 0.8908, Test Accuracy: 0.7644\nTraining completed!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}