{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom collections import Counter\nimport os\n\n# Define transformations for the training and testing sets\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# Load the training and testing datasets with the transformation applied\ntrain_dataset = datasets.ImageFolder(root= \"/kaggle/input/vibeid-a1/A1/train\", transform=transform)\ntest_dataset = datasets.ImageFolder(root= \"/kaggle/input/vibeid-a1/A1/test\", transform=transform)\n\n# Create dataloaders for the datasets\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# # Verify the datasets and dataloaders\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of testing samples: {len(test_dataset)}\")\n\n# Function to count the samples per class\ndef count_samples_per_class(dataset):\n    class_counts = Counter()\n    for _, label in dataset:\n        class_counts[label] += 1\n    return class_counts\n\n# Count the number of samples per class in the training set\ntrain_class_counts = count_samples_per_class(train_dataset)\nprint(\"\\nTraining set class distribution:\")\nfor class_label, count in train_class_counts.items():\n    class_name = train_dataset.classes[class_label]\n    print(f\"Class '{class_name}' ({class_label}): {count} samples\")\n\n# Count the number of samples per class in the test set\ntest_class_counts = count_samples_per_class(test_dataset)\nprint(\"\\nTest set class distribution:\")\nfor class_label, count in test_class_counts.items():\n    class_name = test_dataset.classes[class_label]\n    print(f\"Class '{class_name}' ({class_label}): {count} samples\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets, models\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nmodel = models.resnet50(pretrained=False)\nfor param in model.parameters():\n    param.requires_grad = True\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Linear(num_ftrs, 100)  # Original fully connected layer\n) # Assuming you have 2 classes\ncustom_resnet = model\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(custom_resnet.parameters())\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=3, verbose=True)\n# optimizer =  optim.Adam(custom_resnet.parameters())\n# Train the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncustom_resnet.to(device)\nfrom torch.optim import lr_scheduler\n# Define a learning rate scheduler\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\nnum_epochs = 100\ndef train_and_test_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs):\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()  # Set the model to training mode\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_acc = correct / total\n\n#         print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}\")\n\n        # Testing phase\n        model.eval()  # Set the model to evaluation mode\n        test_correct = 0\n        test_total = 0\n        test_loss = 0.0\n\n        with torch.no_grad():  # Enable gradient computation during inference\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                test_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                test_total += labels.size(0)\n                test_correct += (predicted == labels).sum().item()\n\n        test_loss /= len(test_loader)\n        test_accuracy = test_correct / test_total\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n\n        # Update learning rate scheduler\n        if scheduler is not None:\n            scheduler.step(test_loss)\n\n    print(\"Training completed!\")\n\n\n# Train and test the model\ntrain_and_test_model(custom_resnet, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs)\n\nprint(\"Training completed!\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}